{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_training_2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppiont/tensor-flow-state/blob/master/Model_training_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdvSrOgwIHJ",
        "colab_type": "text"
      },
      "source": [
        "### TRY:\n",
        " - Dropout to 0.2/0.5 instead (*)\n",
        " - Bidirectional LSTM (*)\n",
        " - Learning rate scheduling (and/or stop-resume)\n",
        " - TimeDistributed (Conv1D) or ConvLSTM\n",
        " - exponential smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSuzkeZcM2UE",
        "colab_type": "code",
        "outputId": "c4a70474-0d63-40a4-b299-5c6fd33c3e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBC4ZppZNC3c",
        "colab_type": "code",
        "outputId": "5417f725-7c28-4a7e-e32c-fbdfaf10e04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/gdrive/My Drive/tensor-flow-state/tensor-flow-state\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/tensor-flow-state/tensor-flow-state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKD3KY8hNEqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/final_data.csv\", index_col = 0, parse_dates = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyqsebC1PLdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df.index > \"2016-06-02\"]\n",
        "df['density'] = (df.flow * 60) / df.speed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzVAi95GfcBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"speed\", \"flow\", \"density\", \"speed_limit\", \"holiday\", \"weekend\"]\n",
        "continuous_cols = [\"speed\", \"flow\", \"density\"]\n",
        "discrete_cols = [\"speed_limit\", \"holiday\", \"weekend\"]\n",
        "df = df[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh4evfBugwJa",
        "colab_type": "text"
      },
      "source": [
        "### First resample to elmininate some noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVkBrcJDhVxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def resample_df(df, freq = \"15T\"):\n",
        "    r_df = df.copy()\n",
        "    r_df = r_df.resample(freq).agg({\n",
        "           \"speed\": np.mean, \"flow\": np.sum, \"density\": np.mean, \"speed_limit\": np.median, \"holiday\": np.median, \"weekend\": np.median})\n",
        "    return r_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJyIniCBhWIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r_df = resample_df(df, freq = \"15T\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JH8cZ2JlgTx",
        "colab_type": "text"
      },
      "source": [
        "### Exponential smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErhpZBDilgY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# r_df_smoothed = r_df.copy()\n",
        "# for col in continuous_cols:\n",
        "#     r_df_smoothed[col] = r_df_smoothed[col].ewm(span = 3).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFntI2qJlpiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, ax = plt.subplots(figsize = (10, 5), dpi = 150)\n",
        "# r_df['speed'][0:300].plot(ax = ax, style = 'r', label = 'Speed')\n",
        "# r_df_smoothed['speed'][0:300].plot(ax = ax, style = 'b', label = ' Exponential moving average')\n",
        "# r_df_15['speed'][0:100].plot(ax = ax, style = 'g', label = 'Speed_15')\n",
        "# r_df_15['speed'].ewm(span = 3).mean()[0:100].plot(ax = ax, style = 'w', label = 'Speed_15_ema')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M66yh3ektP7",
        "colab_type": "text"
      },
      "source": [
        "### Train, val, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSeGBQtNq2GC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_split(split_df):\n",
        "    train = split_df[split_df.index.year < 2019].copy()\n",
        "    val = split_df[len(train): -len(split_df[split_df.index > \"2019-06\"]) - 1].copy()\n",
        "    test = r_df[len(train) + len(val):].copy()\n",
        "    return train, val, test\n",
        "\n",
        "train, val, test = train_split(r_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFm8wSGx5dm-",
        "colab_type": "text"
      },
      "source": [
        "### MinMax Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRSHkyO85jw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "X_train = scaler.transform(train)\n",
        "y_train = X_train[:, 0]\n",
        "X_val = scaler.transform(val)\n",
        "y_val = X_val[:, 0]\n",
        "X_test = scaler.transform(test)\n",
        "y_test = X_test[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODFfW7yFcwlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "lookback = int(7 * 24 * (60 / 15))\n",
        "batch_size = 512\n",
        "\n",
        "train_gen = TimeseriesGenerator(data = X_train, targets = y_train, length = lookback, batch_size = batch_size)\n",
        "val_gen = TimeseriesGenerator(data = X_val, targets = y_val, length = lookback, batch_size = batch_size)\n",
        "test_gen = TimeseriesGenerator(data = X_test, targets = y_test, length = lookback, batch_size = batch_size)\n",
        " \n",
        "#explore_generator = TimeseriesGenerator(X_train, y_train.shift(-forecast+1).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqJorVCYE9vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ML\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime, os\n",
        "#%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPmUXqP0N3rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############## Define Neural Network Class ##############\n",
        "class neural_net(tf.keras.Model):\n",
        "    def __init__(self, lookback = 24 * 4, regularization = 1e-6): # You can choose to have more input here! E.g. number of neurons.\n",
        "        super(neural_net, self).__init__()\n",
        "\n",
        "        # Define l2 regularization\n",
        "        self.regularization = regularization\n",
        "        regu = tf.keras.regularizers.l2(self.regularization)\n",
        "\n",
        "        # Define lookback\n",
        "        self.lookback = lookback\n",
        "\n",
        "        # Define discrete feature layers\n",
        "        self.discrete_flatten = tf.keras.layers.Flatten()\n",
        "        self.discrete_prelu1 = tf.keras.layers.PReLU()\n",
        "        self.discrete_bnorm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.discrete_dense1 = tf.keras.layers.Dense(128, activation = 'linear', use_bias = True, kernel_regularizer =  regu)\n",
        "        self.discrete_prelu2 = tf.keras.layers.PReLU()\n",
        "\n",
        "        # Define time feature layers\n",
        "        # Time 1\n",
        "        self.time1_conv1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = (1), use_bias = True, kernel_regularizer = regu)\n",
        "        self.time1_prelu1 = tf.keras.layers.PReLU()\n",
        "        self.time1_maxpool = tf.keras.layers.MaxPooling1D(pool_size = 4)\n",
        "        self.time1_lstm1 = tf.keras.layers.LSTM(units = 16, dropout = 0.5, recurrent_dropout = 0.5, input_shape = (self.lookback, 16), kernel_regularizer = regu)\n",
        "        self.time1_prelu2 = tf.keras.layers.PReLU()\n",
        "\n",
        "        # Time 2\n",
        "        self.time2_conv1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = (1), use_bias = True, kernel_regularizer = regu)\n",
        "        self.time2_prelu1 = tf.keras.layers.PReLU()\n",
        "        self.time2_maxpool = tf.keras.layers.MaxPooling1D(pool_size = 4)\n",
        "        self.time2_lstm1 = tf.keras.layers.LSTM(units = 16, dropout = 0.5, recurrent_dropout = 0.5, input_shape = (self.lookback, 16), kernel_regularizer = regu)\n",
        "        self.time2_prelu2 = tf.keras.layers.PReLU()\n",
        "\n",
        "        # Define Merged layers\n",
        "        self.bnorm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, use_bias = True, kernel_regularizer = regu)\n",
        "        self.prelu1 = tf.keras.layers.PReLU()\n",
        "        self.bnorm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(1, activation = 'linear', use_bias = True, kernel_regularizer = regu)\n",
        "\n",
        "    # Define the forward propagation\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # Split time and discrete inputs\n",
        "        x_time = inputs[:, :, :-3]\n",
        "        x_discrete = inputs[:, :, -3:]\n",
        "\n",
        "        # Run discrete layers\n",
        "        x_discrete = self.discrete_flatten(x_discrete)\n",
        "        x_discrete = self.discrete_prelu1(x_discrete)\n",
        "        #x_discrete = self.discrete_bnorm1(x_discrete)\n",
        "        x_discrete = self.discrete_dense1(x_discrete)\n",
        "        x_discrete = self.discrete_prelu2(x_discrete)\n",
        "\n",
        "        # Run time layers\n",
        "        x_time1 = self.time1_conv1(x_time)\n",
        "        x_time1 = self.time1_prelu1(x_time1)\n",
        "        x_time1 = self.time1_maxpool(x_time1)\n",
        "        x_time1 = self.time1_lstm1(x_time1)\n",
        "        x_time1 = self.time1_prelu2(x_time1)\n",
        "        \n",
        "        x_time2 = self.time2_conv1(x_time)\n",
        "        x_time2 = self.time2_prelu1(x_time2)\n",
        "        x_time2 = self.time2_maxpool(x_time2)\n",
        "        x_time2 = self.time2_lstm1(x_time2)\n",
        "        x_time2 = self.time2_prelu2(x_time2)\n",
        "\n",
        "        # Concat layers\n",
        "        x = tf.concat([x_time1, x_time2, x_discrete], axis = 1)\n",
        "        x = self.bnorm1(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.prelu1(x)\n",
        "        x = self.bnorm2(x)\n",
        "        x = self.dense2(x)\n",
        "        \n",
        "        # Return output\n",
        "        return x\n",
        "\n",
        "# Create an instance of you neural network model\n",
        "model = neural_net(lookback = lookback, regularization = 1e-7)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n",
        "\n",
        "# Define loss function\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer = optimizer, loss = mse_loss_fn)\n",
        "\n",
        "# Define callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 6, restore_best_weights = True)\n",
        "learning_rate_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq = 0)\n",
        "#%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_YjgA0ZD59Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4748ec1a-ecfc-4509-efb4-3a5fdcaedfd2"
      },
      "source": [
        "# Train model\n",
        "history = model.fit(train_gen, validation_data = val_gen, epochs = 50, callbacks = [early_stopping, learning_rate_reduce]) #, tensorboard_callback])\n",
        "train_loss, val_loss = history.history['loss'], history.history['val_loss']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 176 steps, validate for 27 steps\n",
            "Epoch 1/50\n",
            "176/176 [==============================] - 125s 708ms/step - loss: 0.1448 - val_loss: 0.0136\n",
            "Epoch 2/50\n",
            "176/176 [==============================] - 118s 673ms/step - loss: 0.0059 - val_loss: 0.0066\n",
            "Epoch 3/50\n",
            "176/176 [==============================] - 120s 680ms/step - loss: 0.0042 - val_loss: 0.0062\n",
            "Epoch 4/50\n",
            "176/176 [==============================] - 121s 685ms/step - loss: 0.0035 - val_loss: 0.0042\n",
            "Epoch 5/50\n",
            "176/176 [==============================] - 121s 686ms/step - loss: 0.0031 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "176/176 [==============================] - 121s 685ms/step - loss: 0.0030 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "176/176 [==============================] - 120s 683ms/step - loss: 0.0027 - val_loss: 0.0034\n",
            "Epoch 8/50\n",
            "176/176 [==============================] - 119s 676ms/step - loss: 0.0026 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "176/176 [==============================] - 119s 679ms/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 10/50\n",
            "176/176 [==============================] - 120s 681ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 11/50\n",
            "176/176 [==============================] - 120s 683ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 12/50\n",
            "176/176 [==============================] - 120s 683ms/step - loss: 0.0023 - val_loss: 0.0031\n",
            "Epoch 13/50\n",
            "176/176 [==============================] - 120s 681ms/step - loss: 0.0023 - val_loss: 0.0025\n",
            "Epoch 14/50\n",
            "176/176 [==============================] - 120s 680ms/step - loss: 0.0024 - val_loss: 0.0026\n",
            "Epoch 15/50\n",
            "176/176 [==============================] - 119s 676ms/step - loss: 0.0023 - val_loss: 0.0031\n",
            "Epoch 16/50\n",
            "176/176 [==============================] - 119s 676ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 17/50\n",
            "176/176 [==============================] - 118s 673ms/step - loss: 0.0024 - val_loss: 0.0026\n",
            "Epoch 18/50\n",
            "176/176 [==============================] - 119s 674ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "Epoch 19/50\n",
            "176/176 [==============================] - 119s 676ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 20/50\n",
            "176/176 [==============================] - 120s 680ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 21/50\n",
            "176/176 [==============================] - 120s 683ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - 121s 685ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 23/50\n",
            "176/176 [==============================] - 119s 675ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 24/50\n",
            "176/176 [==============================] - 118s 672ms/step - loss: 0.0017 - val_loss: 0.0020\n",
            "Epoch 25/50\n",
            "176/176 [==============================] - 118s 671ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 26/50\n",
            "176/176 [==============================] - 118s 668ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 27/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.0016"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOvUVaS5LVzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot test and validation loss\n",
        "plt.rcParams[\"figure.dpi\"] = 150\n",
        "plt.rcParams[\"figure.figsize\"] = (4, 3)\n",
        "plt.figure()\n",
        "plt.semilogy(train_loss, lw = 1)\n",
        "plt.semilogy(val_loss, lw = 1)\n",
        "plt.legend(['Train Loss', 'Val Loss'])\n",
        "plt.grid(True)\n",
        "plt.title('Train Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg96cQGwN3vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_pred = model.predict(val_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro9ZbLYMN3zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions = pd.DataFrame(data = np.exp(val_log.iloc[lookback:, 0].values), index = val_log[lookback:].index, columns = ['True speed'])\n",
        "# predictions[\"Predicted speed\"] = np.exp(val_pred)\n",
        "# predictions[\"Speed limit\"] =  np.where(val.iloc[lookback:, val.columns.get_loc('speed_limit')] > 0.5, 130, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnskeH-k2V-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = pd.DataFrame(data = r_df.iloc[len(train) + lookback : -len(r_df[r_df.index > \"2019-06\"]) - 1, 0].values, index = val[lookback:].index, columns = ['True speed'])\n",
        "# predictions = pd.DataFrame(data = val.iloc[lookback:, 0].values, index = val[lookback:].index, columns = ['True speed'])\n",
        "fudge = val.copy()\n",
        "fudge.iloc[lookback:, 0] = val_pred\n",
        "predictions[\"Predicted speed\"] = scaler.inverse_transform(fudge)[lookback:, 0]\n",
        "predictions[\"Speed limit\"] =  np.where(val.iloc[lookback:, val.columns.get_loc('speed_limit')] > 0.5, 130, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndpr0s0WN33G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "predictions[825:850].plot(figsize = (10, 5), lw = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGw0tlmWN3-2",
        "colab_type": "code",
        "outputId": "2626d300-6618-4cf7-8a9c-6439048f34d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(f\"Model R^2: {np.mean((predictions['True speed'] - predictions['Predicted speed'])**2)}\")\n",
        "speed = predictions['True speed'].values\n",
        "print(f\"Naive R^2: {np.mean((speed[1:] - speed[:-1])**2)}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model R^2: 19.792203435433585\n",
            "Naive R^2: 24.448094035079595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j9QA1GmCUfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "c0663ef8-b003-4871-b713-d7232c5e500a"
      },
      "source": [
        "#Evaluate on test set\n",
        "test_loss = model.evaluate(test_gen)\n",
        "print('Test Loss: ' + str(test_loss))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "40/40 [==============================] - 6s 149ms/step - loss: 0.0014\n",
            "Test Loss: 0.0014382684916199651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX7s0SEw7vHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}